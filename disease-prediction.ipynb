{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy.stats import mode\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, GridSearchCV\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, classification_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_9611/3474924023.py:15: UserWarning: Matplotlib is currently using agg, which is a non-GUI backend, so cannot show the figure.\n",
      "  plt.show()\n"
     ]
    }
   ],
   "source": [
    "# Reading the train.csv by removing the last column since it's an empty column\n",
    "DATA_PATH = \"/home/shailesh/dataset/Training.csv\"\n",
    "data = pd.read_csv(DATA_PATH).dropna(axis=1)\n",
    "\n",
    "# Checking whether the dataset is balanced or not\n",
    "disease_counts = data[\"prognosis\"].value_counts()\n",
    "temp_df = pd.DataFrame({\n",
    "    \"Disease\": disease_counts.index,\n",
    "    \"Counts\": disease_counts.values\n",
    "})\n",
    "\n",
    "plt.figure(figsize=(18, 8))\n",
    "sns.barplot(x=\"Disease\", y=\"Counts\", data=temp_df)\n",
    "plt.xticks(rotation=90)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: (3936, 132), (3936,)\n",
      "Test: (984, 132), (984,)\n"
     ]
    }
   ],
   "source": [
    "# Encoding the target value into numerical value using LabelEncoder\n",
    "encoder = LabelEncoder()\n",
    "data[\"prognosis\"] = encoder.fit_transform(data[\"prognosis\"])\n",
    "\n",
    "\n",
    "X = data.iloc[:, :-1]\n",
    "y = data.iloc[:, -1]\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=24)\n",
    "\n",
    "print(f\"Train: {X_train.shape}, {y_train.shape}\")\n",
    "print(f\"Test: {X_test.shape}, {y_test.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining scoring metric for k-fold cross-validation\n",
    "def cv_scoring(estimator, X, y):\n",
    "    return accuracy_score(y, estimator.predict(X))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initializing Models\n",
    "models = {\n",
    "    \"SVC\": SVC(),\n",
    "    \"Gaussian NB\": GaussianNB(),\n",
    "    \"Random Forest\": RandomForestClassifier(random_state=18)\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==============================\n",
      "SVC\n",
      "Scores: [1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n",
      "Mean Score: 1.0\n",
      "==============================\n",
      "Gaussian NB\n",
      "Scores: [1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n",
      "Mean Score: 1.0\n",
      "==============================\n",
      "Random Forest\n",
      "Scores: [1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n",
      "Mean Score: 1.0\n"
     ]
    }
   ],
   "source": [
    "# Performing cross-validation and evaluating models\n",
    "for model_name, model in models.items():\n",
    "    scores = cross_val_score(model, X, y, cv=10, n_jobs=-1, scoring=cv_scoring)\n",
    "    print(\"=\" * 30)\n",
    "    print(model_name)\n",
    "    print(f\"Scores: {scores}\")\n",
    "    print(f\"Mean Score: {np.mean(scores)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function for hyperparameter tuning using GridSearchCV\n",
    "def tune_model(model, params, X, y):\n",
    "    grid_search = GridSearchCV(model, param_grid=params, cv=5, scoring='accuracy', n_jobs=-1)\n",
    "    grid_search.fit(X, y)\n",
    "    print(f\"Best Parameters: {grid_search.best_params_}\")\n",
    "    print(f\"Best Score: {grid_search.best_score_}\")\n",
    "    return grid_search.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Parameters: {'max_depth': 10, 'n_estimators': 200}\n",
      "Best Score: 1.0\n"
     ]
    }
   ],
   "source": [
    "# Tuning hyperparameters for Random Forest Classifier\n",
    "rf_params = {\n",
    "    'n_estimators': [100, 200, 300],\n",
    "    'max_depth': [5, 10, None]\n",
    "}\n",
    "best_rf_model = tune_model(RandomForestClassifier(random_state=18), rf_params, X_train, y_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on train data by SVM Classifier: 100.0\n",
      "Accuracy on test data by SVM Classifier: 100.0\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00        32\n",
      "           1       1.00      1.00      1.00        21\n",
      "           2       1.00      1.00      1.00        20\n",
      "           3       1.00      1.00      1.00        23\n",
      "           4       1.00      1.00      1.00        24\n",
      "           5       1.00      1.00      1.00        29\n",
      "           6       1.00      1.00      1.00        32\n",
      "           7       1.00      1.00      1.00        24\n",
      "           8       1.00      1.00      1.00        29\n",
      "           9       1.00      1.00      1.00        24\n",
      "          10       1.00      1.00      1.00        25\n",
      "          11       1.00      1.00      1.00        17\n",
      "          12       1.00      1.00      1.00        21\n",
      "          13       1.00      1.00      1.00        27\n",
      "          14       1.00      1.00      1.00        20\n",
      "          15       1.00      1.00      1.00        25\n",
      "          16       1.00      1.00      1.00        23\n",
      "          17       1.00      1.00      1.00        22\n",
      "          18       1.00      1.00      1.00        22\n",
      "          19       1.00      1.00      1.00        28\n",
      "          20       1.00      1.00      1.00        29\n",
      "          21       1.00      1.00      1.00        28\n",
      "          22       1.00      1.00      1.00        26\n",
      "          23       1.00      1.00      1.00        23\n",
      "          24       1.00      1.00      1.00        20\n",
      "          25       1.00      1.00      1.00        21\n",
      "          26       1.00      1.00      1.00        31\n",
      "          27       1.00      1.00      1.00        24\n",
      "          28       1.00      1.00      1.00        21\n",
      "          29       1.00      1.00      1.00        20\n",
      "          30       1.00      1.00      1.00        25\n",
      "          31       1.00      1.00      1.00        25\n",
      "          32       1.00      1.00      1.00        25\n",
      "          33       1.00      1.00      1.00        28\n",
      "          34       1.00      1.00      1.00        25\n",
      "          35       1.00      1.00      1.00        16\n",
      "          36       1.00      1.00      1.00        19\n",
      "          37       1.00      1.00      1.00        24\n",
      "          38       1.00      1.00      1.00        26\n",
      "          39       1.00      1.00      1.00        18\n",
      "          40       1.00      1.00      1.00        22\n",
      "\n",
      "    accuracy                           1.00       984\n",
      "   macro avg       1.00      1.00      1.00       984\n",
      "weighted avg       1.00      1.00      1.00       984\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Training and testing SVM Classifier\n",
    "svm_model = SVC()\n",
    "svm_model.fit(X_train, y_train)\n",
    "svm_preds = svm_model.predict(X_test)\n",
    "\n",
    "print(f\"Accuracy on train data by SVM Classifier: {accuracy_score(y_train, svm_model.predict(X_train)) * 100}\")\n",
    "print(f\"Accuracy on test data by SVM Classifier: {accuracy_score(y_test, svm_preds) * 100}\")\n",
    "print(classification_report(y_test, svm_preds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on train data by Naive Bayes Classifier: 100.0\n",
      "Accuracy on test data by Naive Bayes Classifier: 100.0\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00        32\n",
      "           1       1.00      1.00      1.00        21\n",
      "           2       1.00      1.00      1.00        20\n",
      "           3       1.00      1.00      1.00        23\n",
      "           4       1.00      1.00      1.00        24\n",
      "           5       1.00      1.00      1.00        29\n",
      "           6       1.00      1.00      1.00        32\n",
      "           7       1.00      1.00      1.00        24\n",
      "           8       1.00      1.00      1.00        29\n",
      "           9       1.00      1.00      1.00        24\n",
      "          10       1.00      1.00      1.00        25\n",
      "          11       1.00      1.00      1.00        17\n",
      "          12       1.00      1.00      1.00        21\n",
      "          13       1.00      1.00      1.00        27\n",
      "          14       1.00      1.00      1.00        20\n",
      "          15       1.00      1.00      1.00        25\n",
      "          16       1.00      1.00      1.00        23\n",
      "          17       1.00      1.00      1.00        22\n",
      "          18       1.00      1.00      1.00        22\n",
      "          19       1.00      1.00      1.00        28\n",
      "          20       1.00      1.00      1.00        29\n",
      "          21       1.00      1.00      1.00        28\n",
      "          22       1.00      1.00      1.00        26\n",
      "          23       1.00      1.00      1.00        23\n",
      "          24       1.00      1.00      1.00        20\n",
      "          25       1.00      1.00      1.00        21\n",
      "          26       1.00      1.00      1.00        31\n",
      "          27       1.00      1.00      1.00        24\n",
      "          28       1.00      1.00      1.00        21\n",
      "          29       1.00      1.00      1.00        20\n",
      "          30       1.00      1.00      1.00        25\n",
      "          31       1.00      1.00      1.00        25\n",
      "          32       1.00      1.00      1.00        25\n",
      "          33       1.00      1.00      1.00        28\n",
      "          34       1.00      1.00      1.00        25\n",
      "          35       1.00      1.00      1.00        16\n",
      "          36       1.00      1.00      1.00        19\n",
      "          37       1.00      1.00      1.00        24\n",
      "          38       1.00      1.00      1.00        26\n",
      "          39       1.00      1.00      1.00        18\n",
      "          40       1.00      1.00      1.00        22\n",
      "\n",
      "    accuracy                           1.00       984\n",
      "   macro avg       1.00      1.00      1.00       984\n",
      "weighted avg       1.00      1.00      1.00       984\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Training and testing Naive Bayes Classifier\n",
    "nb_model = GaussianNB()\n",
    "nb_model.fit(X_train, y_train)\n",
    "nb_preds = nb_model.predict(X_test)\n",
    "\n",
    "print(f\"Accuracy on train data by Naive Bayes Classifier: {accuracy_score(y_train, nb_model.predict(X_train)) * 100}\")\n",
    "print(f\"Accuracy on test data by Naive Bayes Classifier: {accuracy_score(y_test, nb_preds) * 100}\")\n",
    "print(classification_report(y_test, nb_preds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on train data by Random Forest Classifier: 100.0\n",
      "Accuracy on test data by Random Forest Classifier: 100.0\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00        32\n",
      "           1       1.00      1.00      1.00        21\n",
      "           2       1.00      1.00      1.00        20\n",
      "           3       1.00      1.00      1.00        23\n",
      "           4       1.00      1.00      1.00        24\n",
      "           5       1.00      1.00      1.00        29\n",
      "           6       1.00      1.00      1.00        32\n",
      "           7       1.00      1.00      1.00        24\n",
      "           8       1.00      1.00      1.00        29\n",
      "           9       1.00      1.00      1.00        24\n",
      "          10       1.00      1.00      1.00        25\n",
      "          11       1.00      1.00      1.00        17\n",
      "          12       1.00      1.00      1.00        21\n",
      "          13       1.00      1.00      1.00        27\n",
      "          14       1.00      1.00      1.00        20\n",
      "          15       1.00      1.00      1.00        25\n",
      "          16       1.00      1.00      1.00        23\n",
      "          17       1.00      1.00      1.00        22\n",
      "          18       1.00      1.00      1.00        22\n",
      "          19       1.00      1.00      1.00        28\n",
      "          20       1.00      1.00      1.00        29\n",
      "          21       1.00      1.00      1.00        28\n",
      "          22       1.00      1.00      1.00        26\n",
      "          23       1.00      1.00      1.00        23\n",
      "          24       1.00      1.00      1.00        20\n",
      "          25       1.00      1.00      1.00        21\n",
      "          26       1.00      1.00      1.00        31\n",
      "          27       1.00      1.00      1.00        24\n",
      "          28       1.00      1.00      1.00        21\n",
      "          29       1.00      1.00      1.00        20\n",
      "          30       1.00      1.00      1.00        25\n",
      "          31       1.00      1.00      1.00        25\n",
      "          32       1.00      1.00      1.00        25\n",
      "          33       1.00      1.00      1.00        28\n",
      "          34       1.00      1.00      1.00        25\n",
      "          35       1.00      1.00      1.00        16\n",
      "          36       1.00      1.00      1.00        19\n",
      "          37       1.00      1.00      1.00        24\n",
      "          38       1.00      1.00      1.00        26\n",
      "          39       1.00      1.00      1.00        18\n",
      "          40       1.00      1.00      1.00        22\n",
      "\n",
      "    accuracy                           1.00       984\n",
      "   macro avg       1.00      1.00      1.00       984\n",
      "weighted avg       1.00      1.00      1.00       984\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Training and testing Random Forest Classifier with tuned hyperparameters\n",
    "best_rf_model.fit(X_train, y_train)\n",
    "rf_preds = best_rf_model.predict(X_test)\n",
    "\n",
    "print(f\"Accuracy on train data by Random Forest Classifier: {accuracy_score(y_train, best_rf_model.predict(X_train)) * 100}\")\n",
    "print(f\"Accuracy on test data by Random Forest Classifier: {accuracy_score(y_test, rf_preds) * 100}\")\n",
    "print(classification_report(y_test, rf_preds))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on train data by Random Forest Classifier: 100.0\n",
      "Accuracy on test data by Random Forest Classifier: 100.0\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00        32\n",
      "           1       1.00      1.00      1.00        21\n",
      "           2       1.00      1.00      1.00        20\n",
      "           3       1.00      1.00      1.00        23\n",
      "           4       1.00      1.00      1.00        24\n",
      "           5       1.00      1.00      1.00        29\n",
      "           6       1.00      1.00      1.00        32\n",
      "           7       1.00      1.00      1.00        24\n",
      "           8       1.00      1.00      1.00        29\n",
      "           9       1.00      1.00      1.00        24\n",
      "          10       1.00      1.00      1.00        25\n",
      "          11       1.00      1.00      1.00        17\n",
      "          12       1.00      1.00      1.00        21\n",
      "          13       1.00      1.00      1.00        27\n",
      "          14       1.00      1.00      1.00        20\n",
      "          15       1.00      1.00      1.00        25\n",
      "          16       1.00      1.00      1.00        23\n",
      "          17       1.00      1.00      1.00        22\n",
      "          18       1.00      1.00      1.00        22\n",
      "          19       1.00      1.00      1.00        28\n",
      "          20       1.00      1.00      1.00        29\n",
      "          21       1.00      1.00      1.00        28\n",
      "          22       1.00      1.00      1.00        26\n",
      "          23       1.00      1.00      1.00        23\n",
      "          24       1.00      1.00      1.00        20\n",
      "          25       1.00      1.00      1.00        21\n",
      "          26       1.00      1.00      1.00        31\n",
      "          27       1.00      1.00      1.00        24\n",
      "          28       1.00      1.00      1.00        21\n",
      "          29       1.00      1.00      1.00        20\n",
      "          30       1.00      1.00      1.00        25\n",
      "          31       1.00      1.00      1.00        25\n",
      "          32       1.00      1.00      1.00        25\n",
      "          33       1.00      1.00      1.00        28\n",
      "          34       1.00      1.00      1.00        25\n",
      "          35       1.00      1.00      1.00        16\n",
      "          36       1.00      1.00      1.00        19\n",
      "          37       1.00      1.00      1.00        24\n",
      "          38       1.00      1.00      1.00        26\n",
      "          39       1.00      1.00      1.00        18\n",
      "          40       1.00      1.00      1.00        22\n",
      "\n",
      "    accuracy                           1.00       984\n",
      "   macro avg       1.00      1.00      1.00       984\n",
      "weighted avg       1.00      1.00      1.00       984\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Training and testing Random Forest Classifier with tuned hyperparameters\n",
    "best_rf_model.fit(X_train, y_train)\n",
    "rf_preds = best_rf_model.predict(X_test)\n",
    "\n",
    "print(f\"Accuracy on train data by Random Forest Classifier: {accuracy_score(y_train, best_rf_model.predict(X_train)) * 100}\")\n",
    "print(f\"Accuracy on test data by Random Forest Classifier: {accuracy_score(y_test, rf_preds) * 100}\")\n",
    "print(classification_report(y_test, rf_preds))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to predict disease based on selected symptoms\n",
    "def predictDisease(selected_symptoms):\n",
    "    # Creating input data for the models\n",
    "    input_data = pd.DataFrame(columns=X.columns)\n",
    "    input_data.loc[0] = 0\n",
    "    for symptom in selected_symptoms:\n",
    "        symptom = symptom.strip().lower()\n",
    "        if symptom in input_data.columns:\n",
    "            input_data[symptom] = 1\n",
    "\n",
    "    # Generating individual outputs\n",
    "    svm_prediction = data_dict[\"predictions_classes\"][final_svm_model.predict(input_data)[0]]\n",
    "    nb_prediction = data_dict[\"predictions_classes\"][final_nb_model.predict(input_data)[0]]\n",
    "    rf_prediction = data_dict[\"predictions_classes\"][final_rf_model.predict(input_data)[0]]\n",
    "\n",
    "    # Making final prediction by taking mode of all predictions\n",
    "    final_prediction = mode([svm_prediction, nb_prediction, rf_prediction])[0][0]\n",
    "    predictions = f\"SVM Model Prediction: {svm_prediction}\\n\" \\\n",
    "                  f\"Naive Bayes Model Prediction: {nb_prediction}\\n\" \\\n",
    "                  f\"Random Forest Model Prediction: {rf_prediction}\\n\" \\\n",
    "                  f\"Final Prediction: {final_prediction}\"\n",
    "    return predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a list of symptom options for the dropdown\n",
    "symptom_options = [symptom.capitalize() for symptom in X.columns.values]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/shailesh/yes/envs/tf/lib/python3.9/site-packages/gradio/inputs.py:151: UserWarning: Usage of gradio.inputs is deprecated, and will not be supported in the future, please import your component from gradio.components\n",
      "  warnings.warn(\n",
      "/home/shailesh/yes/envs/tf/lib/python3.9/site-packages/gradio/inputs.py:154: UserWarning: `optional` parameter is deprecated, and it has no effect\n",
      "  super().__init__(\n",
      "/home/shailesh/yes/envs/tf/lib/python3.9/site-packages/gradio/outputs.py:22: UserWarning: Usage of gradio.outputs is deprecated, and will not be supported in the future, please import your components from gradio.components\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running on local URL:  http://127.0.0.1:7861\n",
      "\n",
      "To create a public link, set `share=True` in `launch()`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"http://127.0.0.1:7861/\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_9611/1430461960.py:17: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  final_prediction = mode([svm_prediction, nb_prediction, rf_prediction])[0][0]\n",
      "/tmp/ipykernel_9611/1430461960.py:17: DeprecationWarning: Support for non-numeric arrays has been deprecated as of SciPy 1.9.0 and will be removed in 1.11.0. `pandas.DataFrame.mode` can be used instead, see https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.mode.html.\n",
      "  final_prediction = mode([svm_prediction, nb_prediction, rf_prediction])[0][0]\n",
      "/tmp/ipykernel_9611/1430461960.py:17: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  final_prediction = mode([svm_prediction, nb_prediction, rf_prediction])[0][0]\n",
      "/tmp/ipykernel_9611/1430461960.py:17: DeprecationWarning: Support for non-numeric arrays has been deprecated as of SciPy 1.9.0 and will be removed in 1.11.0. `pandas.DataFrame.mode` can be used instead, see https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.mode.html.\n",
      "  final_prediction = mode([svm_prediction, nb_prediction, rf_prediction])[0][0]\n",
      "/tmp/ipykernel_9611/1430461960.py:17: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  final_prediction = mode([svm_prediction, nb_prediction, rf_prediction])[0][0]\n",
      "/tmp/ipykernel_9611/1430461960.py:17: DeprecationWarning: Support for non-numeric arrays has been deprecated as of SciPy 1.9.0 and will be removed in 1.11.0. `pandas.DataFrame.mode` can be used instead, see https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.mode.html.\n",
      "  final_prediction = mode([svm_prediction, nb_prediction, rf_prediction])[0][0]\n",
      "/tmp/ipykernel_9611/1430461960.py:17: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  final_prediction = mode([svm_prediction, nb_prediction, rf_prediction])[0][0]\n",
      "/tmp/ipykernel_9611/1430461960.py:17: DeprecationWarning: Support for non-numeric arrays has been deprecated as of SciPy 1.9.0 and will be removed in 1.11.0. `pandas.DataFrame.mode` can be used instead, see https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.mode.html.\n",
      "  final_prediction = mode([svm_prediction, nb_prediction, rf_prediction])[0][0]\n"
     ]
    }
   ],
   "source": [
    "# Create a Gradio interface\n",
    "import gradio as gr\n",
    "\n",
    "iface = gr.Interface(\n",
    "    fn=predictDisease,\n",
    "    inputs=gr.inputs.CheckboxGroup(choices=symptom_options, label=\"Symptoms\"),\n",
    "    outputs=gr.outputs.Textbox(label=\"Predictions\"),\n",
    "    title=\"Disease Predictor\"\n",
    ")\n",
    "\n",
    "# Launch the interface\n",
    "iface.launch()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
